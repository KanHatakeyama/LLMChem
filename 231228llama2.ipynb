{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Q)分子構造+(R)理由+(A)物性データセットのLLMによる学習と予測\n",
    "- Q&A: 融点データセットを使用\n",
    "- R: GPT4を使い､Q&Aをもとに自動生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.35.0\n",
    "#!pip install peft==0.5.0\n",
    "#!pip install bitsandbytes==0.41.1\n",
    "#!pip install accelerate==0.23.0\n",
    "#!pip install flash-attn==2.3.1.post1\n",
    "#!pip install datasets==2.14.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer,pipeline\n",
    "from datasets import Dataset\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "#問題設定: はじめのN件をテストデータにする\n",
    "n_test=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ハイパラ関連\n",
    "#モデル名\n",
    "model_size=7\n",
    "model_size=13\n",
    "model_size=70\n",
    "model_name=f\"meta-llama/Llama-2-{model_size}b-chat-hf\"\n",
    "\n",
    "#LoRA関連\n",
    "r=16\n",
    "lora_alpha=16\n",
    "bit=16\n",
    "bit=4\n",
    "\n",
    "#LoRAのadapter\n",
    "target_modules= [\n",
    "    #\"embed_tokens\",\n",
    "    \"lm_head\",\n",
    "    #\"q_proj\",\n",
    "    #\"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    #\"down_proj\",\n",
    "]\n",
    "\n",
    "#学習関連\n",
    "gradient_checkpointing =True  #vramの節約をしたい場合\n",
    "gradient_checkpointing =False\n",
    "per_device_train_batch_size=1\n",
    "epochs=3\n",
    "lr=10**-5\n",
    "do_train=True\n",
    "#do_train=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device_map=\"auto\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "def init_model(model_name, r, lora_alpha, target_modules, bit=4):\n",
    "    if bit == 4:\n",
    "        print(\"Using 4-bit mode\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                     quantization_config=bnb_config,\n",
    "                                                     device_map=device_map,\n",
    "                                                     use_flash_attention_2=True,\n",
    "                                                     )\n",
    "    elif bit == 16:\n",
    "        print(\"Using fp16 mode\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                     device_map=device_map,\n",
    "                                                     torch_dtype=torch.float16,\n",
    "                                                     use_flash_attention_2=True,\n",
    "                                                     )\n",
    "    else:\n",
    "        raise ValueError(\"bit must be 4 or 16\")\n",
    "\n",
    "    if len(target_modules)==0:\n",
    "        return model\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=\"CAUSAL_LM\", inference_mode=False, r=r, lora_alpha=lora_alpha,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#モデル初期化\n",
    "if model is None:\n",
    "    model=init_model(model_name, r, lora_alpha, target_modules, bit=bit)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#pipe = pipeline(\"text-generation\", model=model,\n",
    "#                tokenizer=tokenizer, max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセットの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2406, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>smiles</th>\n",
       "      <th>csid</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>Reason</th>\n",
       "      <th>mpC</th>\n",
       "      <th>Prediction(integer)</th>\n",
       "      <th>Abs error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2,3-dibromo-2-methyl-butane</td>\n",
       "      <td>BrC(C)(C)C(Br)C</td>\n",
       "      <td>71392</td>\n",
       "      <td>http://www.srcinc.com/what-we-do/product.aspx?...</td>\n",
       "      <td>PHYSPROP</td>\n",
       "      <td>Starting with butane, which melts around -138....</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5-chlorobenzofuroxan</td>\n",
       "      <td>c1cc2c(cc1Cl)no[n+]2[O-]</td>\n",
       "      <td>123661</td>\n",
       "      <td>http://www.alfa.com/en/GP100W.pgm?DSSTK=A14261</td>\n",
       "      <td>Alfa Aesar</td>\n",
       "      <td>The basic structure for comparison is benzene,...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name                    smiles    csid  \\\n",
       "0  2,3-dibromo-2-methyl-butane           BrC(C)(C)C(Br)C   71392   \n",
       "1         5-chlorobenzofuroxan  c1cc2c(cc1Cl)no[n+]2[O-]  123661   \n",
       "\n",
       "                                                link      source  \\\n",
       "0  http://www.srcinc.com/what-we-do/product.aspx?...    PHYSPROP   \n",
       "1     http://www.alfa.com/en/GP100W.pgm?DSSTK=A14261  Alfa Aesar   \n",
       "\n",
       "                                              Reason   mpC  \\\n",
       "0  Starting with butane, which melts around -138....   7.0   \n",
       "1  The basic structure for comparison is benzene,...  47.0   \n",
       "\n",
       "   Prediction(integer)  Abs error  \n",
       "0                  6.6        0.4  \n",
       "1                 45.0        2.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#データセットの読み込み\n",
    "import pandas as pd\n",
    "import random\n",
    "df=pd.read_csv(\"dataset/231225AutoReasoning/240104best_reason_record.csv\")\n",
    "dataset=df.to_dict(orient=\"records\")\n",
    "random.seed(0)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "print(df.shape)\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "system_prompt=\"You are a professional chemist. Predict the melting point of the following compound.\"\n",
    "\n",
    "\n",
    "def gen_compound_text(chemical_record,\n",
    "    reason=\"\",prediction=\"\"):\n",
    "    name=chemical_record[\"name\"]\n",
    "    smiles=chemical_record[\"smiles\"]\n",
    "    prompt=f\"\"\"\n",
    "#Problem\n",
    "##Name: {name}\n",
    "##SMILES: {smiles}\"\"\"\n",
    "    if reason !=\"\" and prediction!=\"\":\n",
    "        prompt+=f\"\"\"\n",
    "##Reason: {reason}\n",
    "##Prediction: {prediction}\n",
    "\"\"\"\n",
    "    else:\n",
    "        #test mode\n",
    "        prompt+=\"\"\"\n",
    "##Reason: \n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "def generate_question_prompt(dataset,test_id,n_prompt_examples=5):\n",
    "    train_ids=[i for i in range(len(dataset))]\n",
    "    train_ids.remove(test_id)\n",
    "    prompt=\"\"\n",
    "\n",
    "    #train prompt\n",
    "    for _ in range(n_prompt_examples):\n",
    "        id=random.choice(train_ids)\n",
    "        prompt+=gen_compound_text(dataset[id],\n",
    "                                reason=dataset[id][\"Reason\"],\n",
    "                                prediction=dataset[id][\"Prediction(integer)\"])\n",
    "        prompt+=\"\\n\"\n",
    "\n",
    "    #test prompt\n",
    "    prompt+=gen_compound_text(dataset[test_id])\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def prepare_dataset(context_list, tokenizer):\n",
    "    data_list = [{\"text\": i} for i in context_list]\n",
    "    random.shuffle(data_list)\n",
    "\n",
    "    # tokenize\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\"text\": [item[\"text\"] for item in data_list[:]]})\n",
    "    dataset = dataset.map(lambda samples: tokenizer(\n",
    "        samples['text']), batched=True)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2356 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2356/2356 [00:00<00:00, 9791.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_text_list=[]\n",
    "for id in range(len(dataset)):\n",
    "    prompt=gen_compound_text(dataset[id],\n",
    "                                reason=dataset[id][\"Reason\"],\n",
    "                                prediction=dataset[id][\"Prediction(integer)\"])\n",
    "    train_text_list.append(prompt)\n",
    "tokenized_dataset = prepare_dataset(train_text_list[n_test:], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "\n",
      "#Problem\n",
      "##Name: 2-Ethoxy-2-methylpropane\n",
      "##SMILES: CC(C)(C)OCC\n",
      "##Reason: The target compound, 2-Ethoxy-2-methylpropane, can be seen as derived from propane, which has a melting point of -187.7°C. Adding a methyl group generally increases the melting point due to increased molecular weight and van der Waals forces. For propane, adding a methyl group to become isobutane elevates the melting point to -159.6°C, indicating the effect of a methyl group is an increase of roughly +28°C. The presence of an ethoxy group (-OCC2H5) introduces an oxygen atom which can lead to dipole-dipole interactions and potential hydrogen bonding with trace moisture, increasing the melting point as well. The ethoxy group's effect on the melting point would be less straightforward to quantify without empirical data, but we can predict that it will raise the melting point to some extent. Considering the combined effect of an additional methyl group and an ethoxy group on the basic structure of propane, we can expect the overall melting point to be higher than that of propane, but lower than room temperature because the compound remains aliphatic and does not have strong intermolecular forces such as hydrogen bonding among its own molecules.\n",
      "##Prediction: -94.0\n",
      "\n",
      "test\n",
      "\n",
      "#Problem\n",
      "##Name: (1,2,2,3-tetramethylcyclopentyl)methyl 4-aminobenzoate\n",
      "##SMILES: O=C(OCC1(C)CCC(C)C1(C)C)c1ccc(N)cc1\n",
      "##Reason: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check prompt\n",
    "\n",
    "print(\"train\")\n",
    "print(prompt)\n",
    "print(\"test\")\n",
    "t_prompt=gen_compound_text(dataset[0])\n",
    "print(t_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7068' max='7068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7068/7068 2:28:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.875200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.824500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.784700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.779700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.750700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.698700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.689600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.655500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.637900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.601900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.585700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.626100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.599000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.560300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.556100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.585300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.551900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.555100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.529600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.509800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.549200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.484500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.513300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.508400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.476600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.465200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.487400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.468700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.445900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.465100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.481800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.459600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.444900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.452800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.430600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.442700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.451500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.452600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.457500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.434700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.458100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.471300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.426700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.397300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.446600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.420600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.416800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.408400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.421400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.388200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.446700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.452800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.412300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.419600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.412600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#train\n",
    "train_args = transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        #gradient_accumulation_steps=1,\n",
    "        warmup_steps=0,\n",
    "        num_train_epochs=epochs,\n",
    "        learning_rate=lr,\n",
    "        fp16=True,\n",
    "        logging_steps=100,\n",
    "        save_total_limit=1,\n",
    "        output_dir='outputs/'+datetime.now().strftime('%Y%m%d%H%M%S'),\n",
    "        gradient_checkpointing=gradient_checkpointing,\n",
    "    )\n",
    "\n",
    "# trainer\n",
    "#callbacks = [EarlyStoppingCallback()]\n",
    "callbacks = []\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=train_args,\n",
    "    callbacks=callbacks,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "        tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "if do_train:\n",
    "    training_result = trainer.train()\n",
    "    training_result.training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルの保存\n",
    "from datetime import datetime\n",
    "current_datetime = datetime.now()\n",
    "#model.save_pretrained(f\"./outputs/{current_datetime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:34<00:00,  2.29s/it]\n"
     ]
    }
   ],
   "source": [
    "#モデルの読み込み\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "#model = AutoPeftModelForCausalLM.from_pretrained(\"outputs/2024-01-05 01:27:09.882003\",device_map=device_map,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルによる物性値の予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import torch\n",
    "import gc\n",
    "from IPython.display import clear_output\n",
    "model.eval()\n",
    "def gen_text_stop_word(prompt,model,tokenizer,\n",
    "                       device=\"cuda:0\",\n",
    "                       stop_words=[\"#Problem\",\"#Reason\"],\n",
    "                       double_stop_words=[\"#Prediction\"],\n",
    "                       stream=False,\n",
    "                       max_tokens=300):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    # 生成されたテキストを格納する変数\n",
    "    generated_text = \"\"\n",
    "\n",
    "    # トークンを一つずつ生成\n",
    "    for i in range(max_tokens):\n",
    "        # 次のトークンを予測\n",
    "        outputs = model(input_ids)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "        # 生成されたトークンを現在の入力に追加\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "        # 生成されたテキストを更新\n",
    "        generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)[len(prompt):]\n",
    "\n",
    "        if stream:\n",
    "            if i%30==0:\n",
    "                clear_output()\n",
    "            print(generated_text)\n",
    "\n",
    "        # ストップワードのチェック\n",
    "        if any(stop_word in generated_text for stop_word in stop_words):\n",
    "            break\n",
    "\n",
    "        # 2回以上出現したらstopするwordのcheck \n",
    "        stop_flag=False\n",
    "        for check_word in double_stop_words:\n",
    "            count=generated_text.count(check_word)\n",
    "            if count>=2:\n",
    "                stop_flag=True\n",
    "                break\n",
    "        if stop_flag:\n",
    "            break\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "def ask_value(prompt,model,tokenizer):\n",
    "    res=gen_text_stop_word(prompt,model,tokenizer)\n",
    "    #res=pipe(prompt)[0][\"generated_text\"]\n",
    "    print(\"----\")\n",
    "    print(res.strip())\n",
    "\n",
    "    regex_list=[\n",
    "        r\"Prediction:\\s*(\\d+\\.?\\d*)\",\n",
    "        r\"Predicted melting point:\\s*(\\d+\\.?\\d*)\",\n",
    "    ]\n",
    "\n",
    "    value=None\n",
    "    for reg in regex_list:\n",
    "        match = re.search(reg, res)\n",
    "        if match:\n",
    "            value = match.group(1)\n",
    "            break\n",
    "    return res,value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promlem 1 / 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random.seed(0)\n",
    "prediction_results={}\n",
    "\n",
    "#予測時のハイパラ\n",
    "n_prompt_examples=3 #何件の例題をprompt tuningで出すか\n",
    "n_max_trials=3  # 値を返さなかったときの再試行の最大数\n",
    "\n",
    "res_list=[]\n",
    "for test_id in tqdm(range(n_test)):\n",
    "    print(f\"promlem {test_id+1} / {n_test}\")\n",
    "    for _ in range(n_max_trials):\n",
    "        try:\n",
    "            prompt=generate_question_prompt(dataset,test_id,n_prompt_examples=n_prompt_examples)\n",
    "            reason,value=ask_value(prompt,model,tokenizer)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "\n",
    "        if value is not None:\n",
    "            record=copy.deepcopy(dataset[test_id])\n",
    "            record[\"Test (Predicted reason)\"]=reason\n",
    "            record[\"Test (Predicted value)\"]=value\n",
    "            res_list.append(record)\n",
    "            print(\"actual: \",record[\"mpC\"],\"predicted: \", record[\"Test (Predicted value)\"],)\n",
    "            break\n",
    "prediction_results[n_prompt_examples]=res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Test (Predicted value)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m sel_df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(records)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#floatに可能なものは変換\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m sel_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest (Predicted value)\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(\u001b[43msel_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTest (Predicted value)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m sel_df\u001b[38;5;241m=\u001b[39msel_df[sel_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest (Predicted value)\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnotnull()]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sel_df)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/pandas/core/indexes/range.py:418\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Test (Predicted value)'"
     ]
    }
   ],
   "source": [
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "from datetime import datetime\n",
    "import json\n",
    "current_datetime = datetime.now()\n",
    "vmin=-200\n",
    "vmax=300\n",
    "\n",
    "#plot prediction results\n",
    "for n_prompt_examples,records in prediction_results.items():\n",
    "    sel_df=pd.DataFrame(records)\n",
    "    #floatに可能なものは変換\n",
    "    sel_df[\"Test (Predicted value)\"] = pd.to_numeric(sel_df[\"Test (Predicted value)\"], errors='coerce')\n",
    "    sel_df=sel_df[sel_df[\"Test (Predicted value)\"].notnull()]\n",
    "    if len(sel_df)==0:\n",
    "        continue\n",
    "    mse=mean_squared_error(sel_df[\"mpC\"],sel_df[\"Test (Predicted value)\"])\n",
    "\n",
    "    plt.figure()\n",
    "    sns.scatterplot(data=sel_df,x=\"mpC\",y=\"Test (Predicted value)\")\n",
    "    plt.title(f\"n_prompt_examples={n_prompt_examples} MSE={mse:.0f}\")\n",
    "\n",
    "    #x,yの範囲を揃える\n",
    "    plt.xlim(vmin,vmax)\n",
    "    plt.ylim(vmin,vmax)\n",
    "    #対角線を描く\n",
    "    plt.plot([vmin,vmax],[vmin,vmax],color=\"gray\")\n",
    "    formatted_filename = f\"results/model={model_size}_{current_datetime.strftime('%Y%m%d_%H%M%S')}_train={do_train}.png\"\n",
    "    plt.savefig(formatted_filename)\n",
    "    #break\n",
    "\n",
    "save_json_filename=formatted_filename.replace(\".png\",\".json\")\n",
    "with open(save_json_filename,\"w\") as f:\n",
    "    json.dump(prediction_results,fp=f,\n",
    "              indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5895.850222222222\n",
      "54.964444444444446\n",
      "0.30817339249534215\n"
     ]
    }
   ],
   "source": [
    "#スコア\n",
    "print(mean_squared_error(sel_df[\"mpC\"],sel_df[\"Test (Predicted value)\"]))\n",
    "print(mean_absolute_error(sel_df[\"mpC\"],sel_df[\"Test (Predicted value)\"]))\n",
    "print(r2_score(sel_df[\"mpC\"],sel_df[\"Test (Predicted value)\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#回答可能な問題の割合\n",
    "sel_df.shape[0]/n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
