{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Q)分子構造+(R)理由+(A)物性データセットのLLMによる学習と予測\n",
    "- Q&A: 融点データセットを使用\n",
    "- R: GPT4を使い､Q&Aをもとに自動生成\n",
    "- 強化学習をしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.35.0\n",
    "#!pip install peft==0.5.0\n",
    "#!pip install bitsandbytes==0.41.1\n",
    "#!pip install accelerate==0.23.0\n",
    "#!pip install flash-attn==2.3.1.post1\n",
    "#!pip install datasets==2.14.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer,pipeline\n",
    "from datasets import Dataset\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "#問題設定: はじめのN件をテストデータにする\n",
    "n_test=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ハイパラ関連\n",
    "#モデル名\n",
    "model_size=7\n",
    "#model_size=13\n",
    "#model_size=70\n",
    "model_name=f\"meta-llama/Llama-2-{model_size}b-chat-hf\"\n",
    "\n",
    "#model_name=\"ahxt/LiteLlama-460M-1T\"\n",
    "#model_name = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n",
    "\n",
    "#model_name = 'openlm-research/open_llama_3b_v2'\n",
    "#LoRA関連\n",
    "r=32\n",
    "lora_alpha=r\n",
    "bit=16\n",
    "#bit=8\n",
    "#bit=4\n",
    "\n",
    "#LoRAのadapter\n",
    "target_modules= [\n",
    "    #\"embed_tokens\",\n",
    "    \"lm_head\",\n",
    "    #\"q_proj\",\n",
    "    #\"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    #\"down_proj\",\n",
    "]\n",
    "\n",
    "#学習関連\n",
    "#gradient_checkpointing =True  #vramの節約をしたい場合\n",
    "gradient_checkpointing =False\n",
    "per_device_train_batch_size=1\n",
    "epochs=3\n",
    "lr=10**-5\n",
    "do_train=True\n",
    "#do_train=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device_map=\"auto\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "def init_model(model_name, r, lora_alpha, target_modules, bit=4):\n",
    "    if bit == 4:\n",
    "        print(\"Using 4-bit mode\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                     quantization_config=bnb_config,\n",
    "                                                     device_map=device_map,\n",
    "                                                     use_flash_attention_2=True,\n",
    "                                                     )\n",
    "    elif bit == 8:\n",
    "        print(\"Using 8-bit mode\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                     load_in_8bit=True,\n",
    "                                                     device_map=device_map,\n",
    "                                                     use_flash_attention_2=True,\n",
    "                                                     )\n",
    "    elif bit == 16:\n",
    "        print(\"Using fp16 mode\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                     device_map=device_map,\n",
    "                                                     torch_dtype=torch.float16,\n",
    "                                                     use_flash_attention_2=True,\n",
    "                                                     )\n",
    "    else:\n",
    "        raise ValueError(\"bit must be 4 or 16\")\n",
    "\n",
    "    if len(target_modules)==0:\n",
    "        return model\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=\"CAUSAL_LM\", inference_mode=False, r=r, lora_alpha=lora_alpha,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fp16 mode\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#モデル初期化\n",
    "model=init_model(model_name, r, lora_alpha, target_modules, bit=bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#pipe = pipeline(\"text-generation\", model=model,\n",
    "#                tokenizer=tokenizer, max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセットの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2406, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>smiles</th>\n",
       "      <th>csid</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>Reason</th>\n",
       "      <th>mpC</th>\n",
       "      <th>Prediction(integer)</th>\n",
       "      <th>Abs error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2,3-dibromo-2-methyl-butane</td>\n",
       "      <td>BrC(C)(C)C(Br)C</td>\n",
       "      <td>71392</td>\n",
       "      <td>http://www.srcinc.com/what-we-do/product.aspx?...</td>\n",
       "      <td>PHYSPROP</td>\n",
       "      <td>Starting with butane, which melts around -138....</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5-chlorobenzofuroxan</td>\n",
       "      <td>c1cc2c(cc1Cl)no[n+]2[O-]</td>\n",
       "      <td>123661</td>\n",
       "      <td>http://www.alfa.com/en/GP100W.pgm?DSSTK=A14261</td>\n",
       "      <td>Alfa Aesar</td>\n",
       "      <td>The basic structure for comparison is benzene,...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name                    smiles    csid  \\\n",
       "0  2,3-dibromo-2-methyl-butane           BrC(C)(C)C(Br)C   71392   \n",
       "1         5-chlorobenzofuroxan  c1cc2c(cc1Cl)no[n+]2[O-]  123661   \n",
       "\n",
       "                                                link      source  \\\n",
       "0  http://www.srcinc.com/what-we-do/product.aspx?...    PHYSPROP   \n",
       "1     http://www.alfa.com/en/GP100W.pgm?DSSTK=A14261  Alfa Aesar   \n",
       "\n",
       "                                              Reason   mpC  \\\n",
       "0  Starting with butane, which melts around -138....   7.0   \n",
       "1  The basic structure for comparison is benzene,...  47.0   \n",
       "\n",
       "   Prediction(integer)  Abs error  \n",
       "0                  6.6        0.4  \n",
       "1                 45.0        2.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#データセットの読み込み\n",
    "import pandas as pd\n",
    "import random\n",
    "df=pd.read_csv(\"dataset/231225AutoReasoning/240104best_reason_record.csv\")\n",
    "dataset=df.to_dict(orient=\"records\")\n",
    "random.seed(0)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "print(df.shape)\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "system_prompt=\"You are a professional chemist. Predict the melting point of the following compound.\"\n",
    "\n",
    "\n",
    "def gen_compound_text(chemical_record,\n",
    "    reason=\"\",prediction=\"\"):\n",
    "    name=chemical_record[\"name\"]\n",
    "    smiles=chemical_record[\"smiles\"]\n",
    "    prompt=f\"\"\"\n",
    "#Problem\n",
    "##Name: {name}\n",
    "##SMILES: {smiles}\"\"\"\n",
    "    if reason !=\"\" and prediction!=\"\":\n",
    "        prompt+=f\"\"\"\n",
    "##Reason: {reason}\n",
    "##Prediction: {prediction}\n",
    "\"\"\"\n",
    "    else:\n",
    "        #test mode\n",
    "        prompt+=\"\"\"\n",
    "##Reason: \n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "def generate_question_prompt(dataset,test_id,n_prompt_examples=5):\n",
    "    train_ids=[i for i in range(len(dataset))]\n",
    "    train_ids.remove(test_id)\n",
    "    prompt=\"\"\n",
    "\n",
    "    #train prompt\n",
    "    for _ in range(n_prompt_examples):\n",
    "        id=random.choice(train_ids)\n",
    "        prompt+=gen_compound_text(dataset[id],\n",
    "                                reason=dataset[id][\"Reason\"],\n",
    "                                prediction=dataset[id][\"Prediction(integer)\"])\n",
    "        prompt+=\"\\n\"\n",
    "\n",
    "    #test prompt\n",
    "    prompt+=gen_compound_text(dataset[test_id])\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def prepare_dataset(context_list, tokenizer):\n",
    "    data_list = [{\"text\": i} for i in context_list]\n",
    "    random.shuffle(data_list)\n",
    "\n",
    "    # tokenize\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\"text\": [item[\"text\"] for item in data_list[:]]})\n",
    "    dataset = dataset.map(lambda samples: tokenizer(\n",
    "        samples['text']), batched=True)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2356/2356 [00:00<00:00, 8620.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_text_list=[]\n",
    "for id in range(len(dataset)):\n",
    "    prompt=gen_compound_text(dataset[id],\n",
    "                                reason=dataset[id][\"Reason\"],\n",
    "                                prediction=dataset[id][\"Prediction(integer)\"])\n",
    "    train_text_list.append(prompt)\n",
    "tokenized_dataset = prepare_dataset(train_text_list[n_test:], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "\n",
      "#Problem\n",
      "##Name: 2-Ethoxy-2-methylpropane\n",
      "##SMILES: CC(C)(C)OCC\n",
      "##Reason: The target compound, 2-Ethoxy-2-methylpropane, can be seen as derived from propane, which has a melting point of -187.7°C. Adding a methyl group generally increases the melting point due to increased molecular weight and van der Waals forces. For propane, adding a methyl group to become isobutane elevates the melting point to -159.6°C, indicating the effect of a methyl group is an increase of roughly +28°C. The presence of an ethoxy group (-OCC2H5) introduces an oxygen atom which can lead to dipole-dipole interactions and potential hydrogen bonding with trace moisture, increasing the melting point as well. The ethoxy group's effect on the melting point would be less straightforward to quantify without empirical data, but we can predict that it will raise the melting point to some extent. Considering the combined effect of an additional methyl group and an ethoxy group on the basic structure of propane, we can expect the overall melting point to be higher than that of propane, but lower than room temperature because the compound remains aliphatic and does not have strong intermolecular forces such as hydrogen bonding among its own molecules.\n",
      "##Prediction: -94.0\n",
      "\n",
      "test\n",
      "\n",
      "#Problem\n",
      "##Name: (1,2,2,3-tetramethylcyclopentyl)methyl 4-aminobenzoate\n",
      "##SMILES: O=C(OCC1(C)CCC(C)C1(C)C)c1ccc(N)cc1\n",
      "##Reason: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check prompt\n",
    "\n",
    "print(\"train\")\n",
    "print(prompt)\n",
    "print(\"test\")\n",
    "t_prompt=gen_compound_text(dataset[0])\n",
    "print(t_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7068' max='7068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7068/7068 08:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.785300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.305500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.282300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.245000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.245000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.229100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.206300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.191600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.204900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.082800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.090600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.096100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.082900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.067800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.095100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.094900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.019400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.044500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.064300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.007700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#train\n",
    "train_args = transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        #gradient_accumulation_steps=1,\n",
    "        warmup_steps=0,\n",
    "        num_train_epochs=epochs,\n",
    "        learning_rate=lr,\n",
    "        fp16=True,\n",
    "        logging_steps=100,\n",
    "        save_total_limit=1,\n",
    "        output_dir='outputs/'+datetime.now().strftime('%Y%m%d%H%M%S'),\n",
    "        gradient_checkpointing=gradient_checkpointing,\n",
    "    )\n",
    "\n",
    "# trainer\n",
    "#callbacks = [EarlyStoppingCallback()]\n",
    "callbacks = []\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=train_args,\n",
    "    callbacks=callbacks,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "        tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "if do_train:\n",
    "    training_result = trainer.train()\n",
    "    training_result.training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルの保存: adapterのみ保存するとき｡\n",
    "from datetime import datetime\n",
    "current_datetime = datetime.now()\n",
    "#model.save_pretrained(f\"./outputs/{current_datetime}\")\n",
    "#model.save_pretrained(f\"./outputs/1b_ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = AutoPeftModelForCausalLM.from_pretrained(model_path,\\n                                                 device_map=device_map,\\n                                                     torch_dtype=torch.float16,\\n                                                     use_flash_attention_2=True,\\n                                                 )\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#モデルの読み込み: 強化学習しない場合\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "model_path=\"./outputs/7b_ft\"\n",
    "\n",
    "\"\"\"\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(model_path,\n",
    "                                                 device_map=device_map,\n",
    "                                                     torch_dtype=torch.float16,\n",
    "                                                     use_flash_attention_2=True,\n",
    "                                                 )\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルのマージと保存: 強化学習するとき\n",
    "#merged_model = model.merge_and_unload()\n",
    "#merged_model.save_pretrained(f\"./outputs/3b_ft_merge_3epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習による追加訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#予測周りのutility funcs\n",
    "import re\n",
    "import torch\n",
    "import gc\n",
    "from IPython.display import clear_output\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "def gen_text_stop_word(prompt,model,tokenizer,\n",
    "                       device=\"cuda:0\",\n",
    "                       stop_words=[\"#Problem\",\"#Reason\",\"# Problem\"],\n",
    "                       double_stop_words=[\"#Prediction\"],\n",
    "                       stream=False,\n",
    "                       #stream=True,\n",
    "                       max_tokens=400,\n",
    "                       ):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    # 生成されたテキストを格納する変数\n",
    "    generated_text = \"\"\n",
    "\n",
    "    # トークンを一つずつ生成\n",
    "    for i in range(max_tokens):\n",
    "        # 次のトークンを予測\n",
    "        outputs = model(input_ids)\n",
    "        if type(model) is AutoModelForCausalLMWithValueHead:\n",
    "            #AutoModelForCausalLMWithValueHeadの場合\n",
    "            logits = outputs[0]\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "        else:\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "        # 生成されたトークンを現在の入力に追加\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "        # 生成されたテキストを更新\n",
    "        generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)[len(prompt):]\n",
    "\n",
    "        if stream:\n",
    "            if i%30==0:\n",
    "                clear_output()\n",
    "            print(generated_text)\n",
    "\n",
    "        # ストップワードのチェック\n",
    "        if any(stop_word in generated_text for stop_word in stop_words):\n",
    "            break\n",
    "\n",
    "        # 2回以上出現したらstopするwordのcheck \n",
    "        stop_flag=False\n",
    "        for check_word in double_stop_words:\n",
    "            count=generated_text.count(check_word)\n",
    "            if count>=2:\n",
    "                stop_flag=True\n",
    "                break\n",
    "        if stop_flag:\n",
    "            break\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "def ask_value(prompt,model,tokenizer):\n",
    "    res=gen_text_stop_word(prompt,model,tokenizer)\n",
    "    #res=pipe(prompt)[0][\"generated_text\"]\n",
    "    print(\"----\\n\\n\")\n",
    "    print(res.strip())\n",
    "\n",
    "    regex_list=[\n",
    "        r\"Prediction:\\s*(\\d+\\.?\\d*)\",\n",
    "        r\"Prediction:\\s*(-?\\d+\\.?\\d*)\",\n",
    "    ]\n",
    "\n",
    "    value=None\n",
    "    for reg in regex_list:\n",
    "        match = re.search(reg, res)\n",
    "        if match:\n",
    "            value = match.group(1)\n",
    "            break\n",
    "\n",
    "\n",
    "    return res,value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig\n",
    "\n",
    "\n",
    "#テキストでファインチューニングしたモデルを強化学習用に読み込み\n",
    "\n",
    "def load_ppo_model(model_path):\n",
    "    lora_config = LoraConfig(\n",
    "    target_modules=target_modules,\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        model_path,\n",
    "        #load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "        peft_config=lora_config,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def init_ppo_trainer(model,tokenizer):\n",
    "\n",
    "    #trainerの定義\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        config=PPOConfig(batch_size=1),\n",
    "        model=model, \n",
    "        #ref_model=ref_model, \n",
    "        tokenizer=tokenizer,\n",
    "        )\n",
    "    return ppo_trainer\n",
    "\n",
    "def reload_ppo_model_and_trainer(model,ppo_trainer,temp_dir=\"outputs/temp\"):\n",
    "    #model.to(\"cuda:0\")\n",
    "    model.save_pretrained(temp_dir)\n",
    "\n",
    "    model=None\n",
    "    ppo_trainer=None\n",
    "    print(\"clearing memory...\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model=load_ppo_model(temp_dir)\n",
    "    ppo_trainer=init_ppo_trainer(model,tokenizer)\n",
    "\n",
    "    return model,ppo_trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#報酬クラスの定義\n",
    "class RewardModel:\n",
    "    def __init__(self,model,dataset,tokenizer,\n",
    "                 range_value=1,\n",
    "                 reward_slope=30,\n",
    "                 n_prompt_examples=3,\n",
    "                 device=\"cuda:0\",\n",
    "                 ):\n",
    "        self.model=model\n",
    "        self.dataset=dataset\n",
    "        self.tokenizer=tokenizer    \n",
    "        self.n_prompt_examples=n_prompt_examples\n",
    "        self.range_value = range_value\n",
    "        self.reward_slope = reward_slope\n",
    "        self.device=device\n",
    "\n",
    "    def calculate_reward(self,abs_error, ):\n",
    "        \"\"\"\n",
    "        Calculate the reward based on the absolute error.\n",
    "\n",
    "        :param abs_error: The absolute error value.\n",
    "        :param range_value: The range value for maximum and minimum rewards.\n",
    "        :return: The calculated reward.\n",
    "        \"\"\"\n",
    "\n",
    "        reward = self.range_value - (abs_error) * (self.range_value / self.reward_slope)\n",
    "        return max(reward, -self.range_value)\n",
    "\n",
    "\n",
    "    def __call__(self,train_id):\n",
    "        prompt=generate_question_prompt(self.dataset,train_id,\n",
    "                                        n_prompt_examples=self.n_prompt_examples)\n",
    "        reason,value=ask_value(prompt,self.model,self.tokenizer,\n",
    "                                         )\n",
    "\n",
    "        actual=self.dataset[train_id][\"mpC\"]\n",
    "        if value is None:\n",
    "            reward=-self.range_value\n",
    "        else:\n",
    "            value=float(value)\n",
    "\n",
    "            abs_error=abs(actual-value)\n",
    "            # Example usage of the function\n",
    "            reward=self.calculate_reward(abs_error)  # Example case where abs_error is 12 and range_value is 100\n",
    "\n",
    "        #説明が殆どない場合は､rewardを0にする\n",
    "        if len(reason)<30:\n",
    "            reward=0\n",
    "        print(f\"actual: {actual}, predicted: {value}, reward: {reward}\")\n",
    "\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "        return reward,reason,input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_path=\"./outputs/13b_ft_merge_3epoch\"\\nmodel=AutoModelForCausalLM.from_pretrained(model_path,device_map=device_map,\\n                                              torch_dtype=torch.float16,\\n                                                     use_flash_attention_2=True,\\n                                           )\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model_path=\"./outputs/13b_ft_merge_3epoch\"\n",
    "model=AutoModelForCausalLM.from_pretrained(model_path,device_map=device_map,\n",
    "                                              torch_dtype=torch.float16,\n",
    "                                                     use_flash_attention_2=True,\n",
    "                                           )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.05it/s]\n",
      "WARNING:root:A <class 'peft.peft_model.PeftModelForCausalLM'> model is loaded from './outputs/7b_ft_merge_3epoch', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:257: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clearing memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:`peft_config` argument ignored since a peft config file was found in outputs/temp\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "#model_path=\"./outputs/13b_ft_merge_3epoch\"\n",
    "#model_path=\"./outputs/3b_ft_merge_3epoch\"\n",
    "model_path=\"./outputs/7b_ft_merge_3epoch\"\n",
    "temp_dir=\"outputs/temp\"\n",
    "\n",
    "#model load\n",
    "model=load_ppo_model(model_path)\n",
    "ppo_trainer=init_ppo_trainer(model,tokenizer)\n",
    "\n",
    "#lora modelとして読み込み直す(vramの節約のため)\n",
    "model,ppo_trainer=reload_ppo_model_and_trainer(model,ppo_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reward_model=None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "reward_model = RewardModel(model,dataset,tokenizer,)\n",
    "reward_model.calculate_reward(30)  #報酬の動作確認. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "The compound (3-phenyl-oxiranyl)-methanol consists of a phenyl ring, an oxiranyl ring, and a methanol group. The phenyl ring, being aromatic, contributes to a higher melting point due to π-π interactions and increased molecular rigidity, estimated at +20°C. The oxiranyl ring, with its oxygen atom capable of hydrogen bonding, significantly increases the melting point, estimated at +100°C. The methanol group, being a small hydrogen bond donor, adds a moderate increase, estimated at +5°C.\n",
      "##Prediction: 135.0\n",
      "\n",
      "\n",
      "#Prediction\n",
      "actual: 26.5, predicted: 135.0, reward: -1\n",
      "----\n",
      "\n",
      "\n",
      "The base structure to consider is benzene, which has a melting point of 5.5 degrees Celsius. The compound in question has a phenyl group, an oxiranyl ring, and a methanol group. The phenyl group increases the melting point due to π-π stacking interactions and increased molecular weight, estimated at +20 degrees Celsius. The oxiranyl ring is a five-membered ring with an oxygen atom that can participate in hydrogen bonding, which would significantly increase the melting point due to the potential for strong intermolecular hydrogen bonding. This could be estimated at +80 degrees Celsius. The methanol group is a small hydrogen bond donor, which might contribute to an increase in melting point, estimated at +5 degrees Celsius. Combining these effects gives us a predicted shift in melting point.\n",
      "##Prediction: 155.0\n",
      "\n",
      "\n",
      "#Prediction\n",
      "actual: 26.5, predicted: 155.0, reward: -1\n",
      "----\n",
      "\n",
      "\n",
      "The compound in question is (3-phenyl-oxiranyl)-methanol. The basic unit, methanol, has a melting point of -95.2°C. The phenyl group is an aromatic ring that increases the melting point due to increased molecular weight and pi-pi interactions, estimated at +20°C. The oxiranyl group is a five-membered ring with an oxygen atom that can participate in hydrogen bonding, which significantly increases the melting point, estimated at +50°C. The methyl group attached to the phenyl ring adds to the molecular weight and van der Waals interactions, estimated at +5°C.\n",
      "##Prediction: -40.0\n",
      "\n",
      "\n",
      "#Prediction\n",
      "actual: 26.5, predicted: -40.0, reward: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/50000 [25:51<1734:04:35, 124.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "The compound in question is a derivative of hydrazine with a benzylidene group and two nitro groups. The basic unit, hydrazine, has a melting point of -60.0°C. The benzylidene group, which is a rigid and bulky structure, can increase the melting point due to increased molecular weight and van der Waals forces, estimated at +20°C. The two nitro groups are highly electron-withdrawing and can form strong intermolecular forces such as dipole-dipole interactions and hydrogen bonding, which would significantly raise the melting point. Each nitro group can be estimated to contribute an increase of about 50°C due to these interactions. The overall predicted increase is therefore the sum of these contributions:\n",
      "\n",
      "Base hydrazine: -60.0\n",
      "Benzylidene group: +20.0\n",
      "Nitro groups: +50.0\n",
      "Total estimated increase: -60.0 + 20.0 + 50.0 = 30.0°C\n",
      "\n",
      "However, the actual melting point is 198.0°C, which suggests that the contribution of the nitro groups is overestimated. The strong electron-withdrawing nature of the nitro groups may not lead to as much increase in melting point as initially predicted. A re-evaluation of the contribution of the nitro groups is necessary to align with the actual value.\n",
      "##Prediction: 198.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual: 240.0, predicted: 198.0, reward: -0.3999999999999999\n",
      "----\n",
      "\n",
      "\n",
      "The compound in question has several functional groups that influence its melting point. The base structure is a hydrazine, which has a melting point of around 20°C. The presence of a benzylidene group, which is a rigid and planar structure, can increase the melting point due to increased molecular packing; this effect can be estimated at +20°C. The nitro groups are highly electron-withdrawing and can engage in strong intermolecular interactions, such as hydrogen bonding and dipole-dipole interactions; each nitro group can be estimated to contribute an increase of +30°C to the melting point. The hydrazine's nitrogen can also participate in hydrogen bonding, which can further increase the melting point; this effect can be estimated at +10°C. The overall effect of these groups is a significant increase in the melting point due to the strong intermolecular forces they introduce.\n",
      "##Prediction: 100.0\n",
      "\n",
      "\n",
      "#Prediction\n",
      "actual: 240.0, predicted: 100.0, reward: -1\n",
      "----\n",
      "\n",
      "\n",
      "The compound in question is a complex molecule with several functional groups that influence its melting point. The base structure is a hydrazine ring, which has a melting point of -6 degrees Celsius. The presence of a benzylidene group (phenylmethylidene) adds rigidity and increases the melting point due to π-π interactions and increased molecular weight (+20). The nitro groups are highly electron-withdrawing and can form strong intermolecular interactions, such as dipole-dipole interactions and potential hydrogen bonding, which significantly raises the melting point (+50 per nitro group). The hydrazine ring itself can form hydrogen bonds, which would further increase the melting point (+20). The overall structure is quite rigid and planar, which usually leads to higher packing efficiency in the solid state.\n",
      "##Prediction: 106.0\n",
      "\n",
      "\n",
      "#Prediction\n",
      "actual: 240.0, predicted: 106.0, reward: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/50000 [28:59<1999:22:02, 143.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "The compound 3-amino-5-methylthio-1H-pyrazole-4-carbonitrile has several functional groups that influence its melting point. Starting with the base structure of pyrazole, which has a melting point around 70°C, we consider the following groups: \n",
      "- Amino group: This group can form hydrogen bonds, which significantly increases the melting point. The amino group is expected to contribute an increase of about 40-50°C. \n",
      "- Methylthio group: This group is similar to a methyl group but includes a sulfur atom, which can engage in dipole-dipole interactions and potentially hydrogen bonding. The methylthio group might add an additional 20-30°C. \n",
      "- Carbonitrile group: This group is polar and can participate in dipole-dipole interactions, potentially adding another 20-30°C. \n",
      "Summing these up, the increase due to the functional groups is estimated to be around 90-150°C above the base value of pyrazole.\n",
      "##Prediction: 165.0\n",
      "\n",
      "\n",
      "#Prediction\n",
      "actual: 149.0, predicted: 165.0, reward: 0.4666666666666667\n",
      "----\n",
      "\n",
      "\n",
      "The target compound has several functional groups that influence its melting point. Starting with pyrazole, which has a melting point of -40°C, the following modifications are considered: 1) The amino group typically increases the melting point due to hydrogen bonding, estimated at +20°C. 2) The methylthio group adds to the molecular weight and can engage in dipole-dipole interactions, estimated at +15°C. 3) The nitrile group is highly polar and can engage in dipole-dipole interactions, estimated at +20°C. 4) The presence of the pyrazole ring itself contributes to the melting point due to its rigid structure and potential for hydrogen bonding, estimated at +30°C. Combining these effects gives a rough estimate of the melting point.\n",
      "##Prediction: 115.0\n",
      "\n",
      "\n",
      "#Prediction\n",
      "actual: 149.0, predicted: 115.0, reward: -0.1333333333333333\n",
      "----\n",
      "\n",
      "\n",
      "The basic unit for comparison is benzene, which has a melting point of 5.5 degrees Celsius. The compound in question has several functional groups that influence the melting point:\n",
      "\n",
      "- The presence of a nitrile group (C#N) generally increases the melting point due to its polarity and potential for dipole-dipole interactions. This might add approximately +20 degrees Celsius.\n",
      "\n",
      "- The amino group (NH2) can form hydrogen bonds, significantly raising the melting point. We might estimate an increase of +50 degrees Celsius.\n",
      "\n",
      "- The methylthio group (CH3S) introduces both van der Waals interactions and the possibility of dipole-dipole interactions, which could contribute an estimated +15 degrees Celsius.\n",
      "\n",
      "- The pyrazole ring introduces rigidity and potential for pi-pi stacking interactions, which might add another +20 degrees Celsius.\n",
      "\n",
      "Summing these estimated contributions, we predict an increase of 105 degrees Celsius relative to benzene. However, it's important to note that these are rough estimations and the actual value may differ due to the complexity of the molecular interactions and the overall molecular structure.\n",
      "##Prediction: 105.0\n",
      "\n",
      "\n",
      "#Prediction\n",
      "actual: 149.0, predicted: 105.0, reward: -0.46666666666666656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/50000 [32:23<1927:37:59, 138.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_retry):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m#報酬､応答､入力の取得\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m         reward,response,input_id\u001b[38;5;241m=\u001b[39m\u001b[43mreward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m         rewards\u001b[38;5;241m=\u001b[39m[torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mfloat\u001b[39m(reward))]\n\u001b[1;32m     33\u001b[0m         query_tensors \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(input_id)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[14], line 33\u001b[0m, in \u001b[0;36mRewardModel.__call__\u001b[0;34m(self, train_id)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,train_id):\n\u001b[1;32m     31\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mgenerate_question_prompt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset,train_id,\n\u001b[1;32m     32\u001b[0m                                     n_prompt_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_prompt_examples)\n\u001b[0;32m---> 33\u001b[0m     reason,value\u001b[38;5;241m=\u001b[39m\u001b[43mask_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     actual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[train_id][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmpC\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[12], line 65\u001b[0m, in \u001b[0;36mask_value\u001b[0;34m(prompt, model, tokenizer)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mask_value\u001b[39m(prompt,model,tokenizer):\n\u001b[0;32m---> 65\u001b[0m     res\u001b[38;5;241m=\u001b[39m\u001b[43mgen_text_stop_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m#res=pipe(prompt)[0][\"generated_text\"]\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 27\u001b[0m, in \u001b[0;36mgen_text_stop_word\u001b[0;34m(prompt, model, tokenizer, device, stop_words, double_stop_words, stream, max_tokens)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# トークンを一つずつ生成\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_tokens):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# 次のトークンを予測\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(model) \u001b[38;5;129;01mis\u001b[39;00m AutoModelForCausalLMWithValueHead:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m#AutoModelForCausalLMWithValueHeadの場合\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/trl/models/modeling_value_head.py:169\u001b[0m, in \u001b[0;36mAutoModelForCausalLMWithValueHead.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_peft_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained_model\u001b[38;5;241m.\u001b[39mactive_peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPREFIX_TUNING\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    167\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 169\u001b[0m base_model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m base_model_output\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    176\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m base_model_output\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/peft/peft_model.py:918\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward in MPTForCausalLM does not support inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[1;32m    909\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    910\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    916\u001b[0m         )\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:94\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1034\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1031\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:922\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    912\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    913\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    914\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m         use_cache,\n\u001b[1;32m    920\u001b[0m     )\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:672\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    671\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    683\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:424\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([F\u001b[38;5;241m.\u001b[39mlinear(attn_output[i], o_proj_slices[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp)])\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 424\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[1;32m    427\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/peft/tuners/lora.py:908\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    902\u001b[0m     result \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlinear(x, transpose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfan_in_fan_out), bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n\u001b[1;32m    904\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter]\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    906\u001b[0m     result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_B[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter](\n\u001b[0;32m--> 908\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_A\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_dropout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    909\u001b[0m         )\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter]\n\u001b[1;32m    911\u001b[0m     )\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    913\u001b[0m     result \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlinear(x, transpose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfan_in_fan_out), bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chemllm/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#model.config.use_cache = False \n",
    "#model.config.use_cache = True\n",
    "lr_epochs=50000\n",
    "n_retry=3\n",
    "#lr_epochs=3\n",
    "reward_log=[]\n",
    "\n",
    "n_iterations=0\n",
    "\n",
    "for i in tqdm(range(lr_epochs)):\n",
    "    if i%5==1:\n",
    "        clear_output()\n",
    "   #ランダムに問題を設定して値を予測させる\n",
    "    train_id=random.randint(n_test,len(dataset))\n",
    "\n",
    "    \n",
    "    #このクラスにgpu cacheが残りがちなので､毎回初期化してメモリ開放しておく\n",
    "    reward_model=None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    reward_model = RewardModel(model,dataset,tokenizer)\n",
    "\n",
    "\n",
    "    #同じ問題に対して､良い結果が来るまで何回か問題を解かせる\n",
    "    for i in range(n_retry):\n",
    "        try:\n",
    "            #報酬､応答､入力の取得\n",
    "            reward,response,input_id=reward_model(train_id)\n",
    "            rewards=[torch.tensor(float(reward))]\n",
    "            query_tensors = [torch.tensor(input_id).reshape(-1)]\n",
    "            response_tensors=[torch.tensor(tokenizer.encode(response)).reshape(-1)]\n",
    "\n",
    "            #負の報酬はあまり頻繁には与えない\n",
    "            if reward<0 and random.random()<0.9:\n",
    "                continue\n",
    "\n",
    "\n",
    "            #モデル更新\n",
    "            stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "            n_iterations+=1\n",
    "            reward_log.append(float(reward))\n",
    "        except Exception as e:\n",
    "            #主にcuda out of memoryが起きるので､一旦メモリ開放して､model類を読み込み直す\n",
    "            print(e)\n",
    "            model,ppo_trainer=reload_ppo_model_and_trainer(model,ppo_trainer)\n",
    "\n",
    "            break\n",
    "\n",
    "        #結果がよかったら次の問題に移る\n",
    "        if reward>0.5:\n",
    "            break\n",
    "\n",
    "n_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5759529480315407"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "#AutoModelForCausalLMWithValueHead classはdirを作っておかないと､save_pretrainedが動かない\n",
    "current_datetime = datetime.now()\n",
    "model_save_path=f\"outputs/ppo{current_datetime}\"\n",
    "os.mkdir(model_save_path)\n",
    "model.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.scatter(range(len(reward_log)),reward_log,s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルによる物性値の予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#予測時のハイパラ\n",
    "n_prompt_examples=3 #何件の例題をprompt tuningで出すか\n",
    "n_max_trials=2  # 値を返さなかったときの再試行の最大数\n",
    "random.seed(0)\n",
    "prediction_results={}\n",
    "\n",
    "\n",
    "res_list=[]\n",
    "for test_id in tqdm(range(n_test)):\n",
    "    print(f\"promlem {test_id+1} / {n_test}\")\n",
    "    for _ in range(n_max_trials):\n",
    "        try:\n",
    "            prompt=generate_question_prompt(dataset,test_id,n_prompt_examples=n_prompt_examples)\n",
    "            reason,value=ask_value(prompt,model,tokenizer)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "\n",
    "        if value is not None:\n",
    "            record=copy.deepcopy(dataset[test_id])\n",
    "            record[\"Test (Predicted reason)\"]=reason\n",
    "            record[\"Test (Predicted value)\"]=value\n",
    "            res_list.append(record)\n",
    "            print(\"actual: \",record[\"mpC\"],\"predicted: \", record[\"Test (Predicted value)\"],)\n",
    "            break\n",
    "prediction_results[n_prompt_examples]=res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "from datetime import datetime\n",
    "import json\n",
    "current_datetime = datetime.now()\n",
    "vmin=-200\n",
    "vmax=300\n",
    "\n",
    "#plot prediction results\n",
    "for n_prompt_examples,records in prediction_results.items():\n",
    "    sel_df=pd.DataFrame(records)\n",
    "    #floatに可能なものは変換\n",
    "    sel_df[\"Test (Predicted value)\"] = pd.to_numeric(sel_df[\"Test (Predicted value)\"], errors='coerce')\n",
    "    sel_df=sel_df[sel_df[\"Test (Predicted value)\"].notnull()]\n",
    "    if len(sel_df)==0:\n",
    "        continue\n",
    "    mse=mean_squared_error(sel_df[\"mpC\"],sel_df[\"Test (Predicted value)\"])\n",
    "\n",
    "    plt.figure()\n",
    "    sns.scatterplot(data=sel_df,x=\"mpC\",y=\"Test (Predicted value)\")\n",
    "    plt.title(f\"n_prompt_examples={n_prompt_examples} MSE={mse:.0f}\")\n",
    "\n",
    "    #x,yの範囲を揃える\n",
    "    plt.xlim(vmin,vmax)\n",
    "    plt.ylim(vmin,vmax)\n",
    "    #対角線を描く\n",
    "    plt.plot([vmin,vmax],[vmin,vmax],color=\"gray\")\n",
    "    formatted_filename = f\"results/model={model_size}_{current_datetime.strftime('%Y%m%d_%H%M%S')}_train={do_train}.png\"\n",
    "    plt.savefig(formatted_filename)\n",
    "    #break\n",
    "\n",
    "save_json_filename=formatted_filename.replace(\".png\",\".json\")\n",
    "with open(save_json_filename,\"w\") as f:\n",
    "    json.dump(prediction_results,fp=f,\n",
    "              indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#スコア\n",
    "print(mean_squared_error(sel_df[\"mpC\"],sel_df[\"Test (Predicted value)\"]))\n",
    "print(mean_absolute_error(sel_df[\"mpC\"],sel_df[\"Test (Predicted value)\"]))\n",
    "print(r2_score(sel_df[\"mpC\"],sel_df[\"Test (Predicted value)\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#回答可能な問題の割合\n",
    "sel_df.shape[0]/n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
