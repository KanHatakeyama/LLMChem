{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Q)分子構造+(R)理由+(A)物性データセットのLLMによる学習と予測\n",
    "- Q&A: 融点データセットを使用\n",
    "- R: GPT4を使い､Q&Aをもとに自動生成\n",
    "- 強化学習をしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.35.0\n",
    "#!pip install peft==0.5.0\n",
    "#!pip install bitsandbytes==0.41.1\n",
    "#!pip install accelerate==0.23.0\n",
    "#!pip install flash-attn==2.3.1.post1\n",
    "#!pip install datasets==2.14.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/setup/miniconda3/envs/chemllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer,pipeline\n",
    "from datasets import Dataset\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "#問題設定: はじめのN件をテストデータにする\n",
    "n_test=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ハイパラ関連\n",
    "#モデル名\n",
    "model_size=7\n",
    "#model_size=13\n",
    "#model_size=70\n",
    "model_name=f\"meta-llama/Llama-2-{model_size}b-chat-hf\"\n",
    "\n",
    "#LoRA関連\n",
    "r=32\n",
    "lora_alpha=r\n",
    "bit=16\n",
    "#bit=8\n",
    "#bit=4\n",
    "\n",
    "#LoRAのadapter\n",
    "target_modules= [\n",
    "    #\"embed_tokens\",\n",
    "    \"lm_head\",\n",
    "    #\"q_proj\",\n",
    "    #\"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    #\"down_proj\",\n",
    "]\n",
    "\n",
    "#学習関連\n",
    "#gradient_checkpointing =True  #vramの節約をしたい場合\n",
    "gradient_checkpointing =False\n",
    "per_device_train_batch_size=1\n",
    "epochs=3\n",
    "lr=10**-5\n",
    "do_train=True\n",
    "#do_train=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device_map=\"auto\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "def init_model(model_name, r, lora_alpha, target_modules, bit=4):\n",
    "    if bit == 4:\n",
    "        print(\"Using 4-bit mode\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                     quantization_config=bnb_config,\n",
    "                                                     device_map=device_map,\n",
    "                                                     use_flash_attention_2=True,\n",
    "                                                     )\n",
    "    elif bit == 8:\n",
    "        print(\"Using 8-bit mode\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                     load_in_8bit=True,\n",
    "                                                     device_map=device_map,\n",
    "                                                     use_flash_attention_2=True,\n",
    "                                                     )\n",
    "    elif bit == 16:\n",
    "        print(\"Using fp16 mode\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                     device_map=device_map,\n",
    "                                                     torch_dtype=torch.float16,\n",
    "                                                     use_flash_attention_2=True,\n",
    "                                                     )\n",
    "    else:\n",
    "        raise ValueError(\"bit must be 4 or 16\")\n",
    "\n",
    "    if len(target_modules)==0:\n",
    "        return model\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=\"CAUSAL_LM\", inference_mode=False, r=r, lora_alpha=lora_alpha,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fp16 mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#モデル初期化\n",
    "model=init_model(model_name, r, lora_alpha, target_modules, bit=bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#pipe = pipeline(\"text-generation\", model=model,\n",
    "#                tokenizer=tokenizer, max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセットの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2406, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>smiles</th>\n",
       "      <th>csid</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>Reason</th>\n",
       "      <th>mpC</th>\n",
       "      <th>Prediction(integer)</th>\n",
       "      <th>Abs error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2,3-dibromo-2-methyl-butane</td>\n",
       "      <td>BrC(C)(C)C(Br)C</td>\n",
       "      <td>71392</td>\n",
       "      <td>http://www.srcinc.com/what-we-do/product.aspx?...</td>\n",
       "      <td>PHYSPROP</td>\n",
       "      <td>Starting with butane, which melts around -138....</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5-chlorobenzofuroxan</td>\n",
       "      <td>c1cc2c(cc1Cl)no[n+]2[O-]</td>\n",
       "      <td>123661</td>\n",
       "      <td>http://www.alfa.com/en/GP100W.pgm?DSSTK=A14261</td>\n",
       "      <td>Alfa Aesar</td>\n",
       "      <td>The basic structure for comparison is benzene,...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name                    smiles    csid  \\\n",
       "0  2,3-dibromo-2-methyl-butane           BrC(C)(C)C(Br)C   71392   \n",
       "1         5-chlorobenzofuroxan  c1cc2c(cc1Cl)no[n+]2[O-]  123661   \n",
       "\n",
       "                                                link      source  \\\n",
       "0  http://www.srcinc.com/what-we-do/product.aspx?...    PHYSPROP   \n",
       "1     http://www.alfa.com/en/GP100W.pgm?DSSTK=A14261  Alfa Aesar   \n",
       "\n",
       "                                              Reason   mpC  \\\n",
       "0  Starting with butane, which melts around -138....   7.0   \n",
       "1  The basic structure for comparison is benzene,...  47.0   \n",
       "\n",
       "   Prediction(integer)  Abs error  \n",
       "0                  6.6        0.4  \n",
       "1                 45.0        2.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#データセットの読み込み\n",
    "import pandas as pd\n",
    "import random\n",
    "df=pd.read_csv(\"dataset/231225AutoReasoning/240104best_reason_record.csv\")\n",
    "dataset=df.to_dict(orient=\"records\")\n",
    "random.seed(0)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "print(df.shape)\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "system_prompt=\"You are a professional chemist. Predict the melting point of the following compound.\"\n",
    "\n",
    "\n",
    "def gen_compound_text(chemical_record,\n",
    "    reason=\"\",prediction=\"\"):\n",
    "    name=chemical_record[\"name\"]\n",
    "    smiles=chemical_record[\"smiles\"]\n",
    "    prompt=f\"\"\"\n",
    "#Problem\n",
    "##Name: {name}\n",
    "##SMILES: {smiles}\"\"\"\n",
    "    if reason !=\"\" and prediction!=\"\":\n",
    "        prompt+=f\"\"\"\n",
    "##Reason: {reason}\n",
    "##Prediction: {prediction}\n",
    "\"\"\"\n",
    "    else:\n",
    "        #test mode\n",
    "        prompt+=\"\"\"\n",
    "##Reason: \n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "def generate_question_prompt(dataset,test_id,n_prompt_examples=5):\n",
    "    train_ids=[i for i in range(len(dataset))]\n",
    "    train_ids.remove(test_id)\n",
    "    prompt=\"\"\n",
    "\n",
    "    #train prompt\n",
    "    for _ in range(n_prompt_examples):\n",
    "        id=random.choice(train_ids)\n",
    "        prompt+=gen_compound_text(dataset[id],\n",
    "                                reason=dataset[id][\"Reason\"],\n",
    "                                prediction=dataset[id][\"Prediction(integer)\"])\n",
    "        prompt+=\"\\n\"\n",
    "\n",
    "    #test prompt\n",
    "    prompt+=gen_compound_text(dataset[test_id])\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def prepare_dataset(context_list, tokenizer):\n",
    "    data_list = [{\"text\": i} for i in context_list]\n",
    "    random.shuffle(data_list)\n",
    "\n",
    "    # tokenize\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\"text\": [item[\"text\"] for item in data_list[:]]})\n",
    "    dataset = dataset.map(lambda samples: tokenizer(\n",
    "        samples['text']), batched=True)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2356 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2356/2356 [00:00<00:00, 8706.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_text_list=[]\n",
    "for id in range(len(dataset)):\n",
    "    prompt=gen_compound_text(dataset[id],\n",
    "                                reason=dataset[id][\"Reason\"],\n",
    "                                prediction=dataset[id][\"Prediction(integer)\"])\n",
    "    train_text_list.append(prompt)\n",
    "tokenized_dataset = prepare_dataset(train_text_list[n_test:], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "\n",
      "#Problem\n",
      "##Name: 2-Ethoxy-2-methylpropane\n",
      "##SMILES: CC(C)(C)OCC\n",
      "##Reason: The target compound, 2-Ethoxy-2-methylpropane, can be seen as derived from propane, which has a melting point of -187.7°C. Adding a methyl group generally increases the melting point due to increased molecular weight and van der Waals forces. For propane, adding a methyl group to become isobutane elevates the melting point to -159.6°C, indicating the effect of a methyl group is an increase of roughly +28°C. The presence of an ethoxy group (-OCC2H5) introduces an oxygen atom which can lead to dipole-dipole interactions and potential hydrogen bonding with trace moisture, increasing the melting point as well. The ethoxy group's effect on the melting point would be less straightforward to quantify without empirical data, but we can predict that it will raise the melting point to some extent. Considering the combined effect of an additional methyl group and an ethoxy group on the basic structure of propane, we can expect the overall melting point to be higher than that of propane, but lower than room temperature because the compound remains aliphatic and does not have strong intermolecular forces such as hydrogen bonding among its own molecules.\n",
      "##Prediction: -94.0\n",
      "\n",
      "test\n",
      "\n",
      "#Problem\n",
      "##Name: (1,2,2,3-tetramethylcyclopentyl)methyl 4-aminobenzoate\n",
      "##SMILES: O=C(OCC1(C)CCC(C)C1(C)C)c1ccc(N)cc1\n",
      "##Reason: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check prompt\n",
    "\n",
    "print(\"train\")\n",
    "print(prompt)\n",
    "print(\"test\")\n",
    "t_prompt=gen_compound_text(dataset[0])\n",
    "print(t_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7068' max='7068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7068/7068 13:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.766400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.247600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.156500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.988400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.955100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.935700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.972700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.949800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.926800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.955100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.966300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.941100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.922900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.926400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.948900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.937300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.934800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.961500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.947200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.939900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.942600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.917200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.930500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.932500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.948400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.892600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.883200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.920400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.906800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.888700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.895400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.912600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.870700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.907100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.914300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.891600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.903900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.864200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.917300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.848700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.917600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.909200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.881100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.860900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#train\n",
    "train_args = transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        #gradient_accumulation_steps=1,\n",
    "        warmup_steps=0,\n",
    "        num_train_epochs=epochs,\n",
    "        learning_rate=lr,\n",
    "        fp16=True,\n",
    "        logging_steps=100,\n",
    "        save_total_limit=1,\n",
    "        output_dir='outputs/'+datetime.now().strftime('%Y%m%d%H%M%S'),\n",
    "        gradient_checkpointing=gradient_checkpointing,\n",
    "    )\n",
    "\n",
    "# trainer\n",
    "#callbacks = [EarlyStoppingCallback()]\n",
    "callbacks = []\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=train_args,\n",
    "    callbacks=callbacks,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "        tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "if do_train:\n",
    "    training_result = trainer.train()\n",
    "    training_result.training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルの保存: adapterのみ保存するとき｡\n",
    "from datetime import datetime\n",
    "current_datetime = datetime.now()\n",
    "#model.save_pretrained(f\"./outputs/{current_datetime}\")\n",
    "#model.save_pretrained(f\"./outputs/7b_ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = AutoPeftModelForCausalLM.from_pretrained(model_path,\\n                                                 device_map=device_map,\\n                                                     torch_dtype=torch.float16,\\n                                                     use_flash_attention_2=True,\\n                                                 )\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#モデルの読み込み: 強化学習しない場合\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "model_path=\"./outputs/7b_ft\"\n",
    "\n",
    "\"\"\"\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(model_path,\n",
    "                                                 device_map=device_map,\n",
    "                                                     torch_dtype=torch.float16,\n",
    "                                                     use_flash_attention_2=True,\n",
    "                                                 )\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルのマージと保存: 強化学習するとき\n",
    "#merged_model = model.merge_and_unload()\n",
    "#merged_model.save_pretrained(f\"./outputs/7b_ft_merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習による追加訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#予測周りのutility funcs\n",
    "import re\n",
    "import torch\n",
    "import gc\n",
    "from IPython.display import clear_output\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "def gen_text_stop_word(prompt,model,tokenizer,\n",
    "                       device=\"cuda:0\",\n",
    "                       stop_words=[\"#Problem\",\"#Reason\",\"# Problem\"],\n",
    "                       double_stop_words=[\"#Prediction\"],\n",
    "                       stream=False,\n",
    "                       #stream=True,\n",
    "                       max_tokens=400,\n",
    "                       ):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    # 生成されたテキストを格納する変数\n",
    "    generated_text = \"\"\n",
    "\n",
    "    # トークンを一つずつ生成\n",
    "    for i in range(max_tokens):\n",
    "        # 次のトークンを予測\n",
    "        outputs = model(input_ids)\n",
    "        if type(model) is AutoModelForCausalLMWithValueHead:\n",
    "            #AutoModelForCausalLMWithValueHeadの場合\n",
    "            logits = outputs[0]\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "        else:\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "        # 生成されたトークンを現在の入力に追加\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "        # 生成されたテキストを更新\n",
    "        generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)[len(prompt):]\n",
    "\n",
    "        if stream:\n",
    "            if i%30==0:\n",
    "                clear_output()\n",
    "            print(generated_text)\n",
    "\n",
    "        # ストップワードのチェック\n",
    "        if any(stop_word in generated_text for stop_word in stop_words):\n",
    "            break\n",
    "\n",
    "        # 2回以上出現したらstopするwordのcheck \n",
    "        stop_flag=False\n",
    "        for check_word in double_stop_words:\n",
    "            count=generated_text.count(check_word)\n",
    "            if count>=2:\n",
    "                stop_flag=True\n",
    "                break\n",
    "        if stop_flag:\n",
    "            break\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "def ask_value(prompt,model,tokenizer):\n",
    "    res=gen_text_stop_word(prompt,model,tokenizer)\n",
    "    #res=pipe(prompt)[0][\"generated_text\"]\n",
    "    print(\"----\\n\\n\")\n",
    "    print(res.strip())\n",
    "\n",
    "    regex_list=[\n",
    "        r\"Prediction:\\s*(\\d+\\.?\\d*)\",\n",
    "        r\"Prediction:\\s*(-?\\d+\\.?\\d*)\",\n",
    "    ]\n",
    "\n",
    "    value=None\n",
    "    for reg in regex_list:\n",
    "        match = re.search(reg, res)\n",
    "        if match:\n",
    "            value = match.group(1)\n",
    "            break\n",
    "\n",
    "\n",
    "    return res,value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig\n",
    "\n",
    "\n",
    "#テキストでファインチューニングしたモデルを強化学習用に読み込み\n",
    "\n",
    "def load_ppo_model(model_path):\n",
    "    lora_config = LoraConfig(\n",
    "    target_modules=target_modules,\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        model_path,\n",
    "        #load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "        peft_config=lora_config,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def init_ppo_trainer(model,tokenizer):\n",
    "\n",
    "    #trainerの定義\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        config=PPOConfig(batch_size=1),\n",
    "        model=model, \n",
    "        #ref_model=ref_model, \n",
    "        tokenizer=tokenizer,\n",
    "        )\n",
    "    return ppo_trainer\n",
    "\n",
    "def reload_ppo_model_and_trainer(model,ppo_trainer,temp_dir=\"outputs/temp\"):\n",
    "\n",
    "    model.save_pretrained(temp_dir)\n",
    "\n",
    "    model=None\n",
    "    ppo_trainer=None\n",
    "    print(\"clearing memory...\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model=load_ppo_model(temp_dir)\n",
    "    ppo_trainer=init_ppo_trainer(model,tokenizer)\n",
    "\n",
    "    return model,ppo_trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.05it/s]\n",
      "WARNING:root:A <class 'peft.peft_model.PeftModelForCausalLM'> model is loaded from './outputs/13b_ft_merge_3epoch', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clearing memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:`peft_config` argument ignored since a peft config file was found in outputs/temp\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path=\"./outputs/7b_ft_merge\"\n",
    "#model_path=\"./outputs/13b_ft_merge_3epoch\"\n",
    "#model_path=\"./outputs/7b_ft_merge_3epoch\"\n",
    "temp_dir=\"outputs/temp\"\n",
    "\n",
    "#model load\n",
    "model=load_ppo_model(model_path)\n",
    "ppo_trainer=init_ppo_trainer(model,tokenizer)\n",
    "\n",
    "#lora modelとして読み込み直す(vramの節約のため)\n",
    "model,ppo_trainer=reload_ppo_model_and_trainer(model,ppo_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model=None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#報酬クラスの定義\n",
    "class RewardModel:\n",
    "    def __init__(self,model,dataset,tokenizer,\n",
    "                 range_value=1,\n",
    "                 reward_slope=50,\n",
    "                 n_prompt_examples=3,\n",
    "                 device=\"cuda:0\",\n",
    "                 ):\n",
    "        self.model=model\n",
    "        self.dataset=dataset\n",
    "        self.tokenizer=tokenizer    \n",
    "        self.n_prompt_examples=n_prompt_examples\n",
    "        self.range_value = range_value\n",
    "        self.reward_slope = reward_slope\n",
    "        self.device=device\n",
    "\n",
    "    def calculate_reward(self,abs_error, ):\n",
    "        \"\"\"\n",
    "        Calculate the reward based on the absolute error.\n",
    "\n",
    "        :param abs_error: The absolute error value.\n",
    "        :param range_value: The range value for maximum and minimum rewards.\n",
    "        :return: The calculated reward.\n",
    "        \"\"\"\n",
    "\n",
    "        reward = self.range_value - (abs_error) * (self.range_value / self.reward_slope)\n",
    "        return max(reward, -self.range_value)\n",
    "\n",
    "\n",
    "    def __call__(self,train_id):\n",
    "        prompt=generate_question_prompt(self.dataset,train_id,\n",
    "                                        n_prompt_examples=self.n_prompt_examples)\n",
    "        reason,value=ask_value(prompt,self.model,self.tokenizer,\n",
    "                                         )\n",
    "\n",
    "        actual=self.dataset[train_id][\"mpC\"]\n",
    "        if value is None:\n",
    "            reward=-self.range_value\n",
    "        else:\n",
    "            value=float(value)\n",
    "\n",
    "            abs_error=abs(actual-value)\n",
    "            # Example usage of the function\n",
    "            reward=self.calculate_reward(abs_error)  # Example case where abs_error is 12 and range_value is 100\n",
    "\n",
    "        #説明が殆どない場合は､rewardを0にする\n",
    "        if len(reason)<30:\n",
    "            reward=0\n",
    "        print(f\"actual: {actual}, predicted: {value}, reward: {reward}\")\n",
    "\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "        return reward,reason,input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reward_model = RewardModel(model,dataset,tokenizer)\n",
    "reward_model.calculate_reward(10)  #報酬の動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "- Base structure, benzene, has a melting point of 5.5°C. \n",
      "- Ketone group (C=O): +40°C, due to the polar nature and the ability to form dipole-dipole interactions. \n",
      "- Methyl group: +5°C, due to an increase in molecular weight and van der Waals forces. \n",
      "- Semicarbazone group: +80°C, due to the ability to form strong hydrogen bonds and the rigidity it introduces to the molecule. \n",
      "The sum of these effects gives us a predicted melting point of 130.5°C.\n",
      "##Prediction: 130.5\n",
      "actual: 136.0, predicted: 130.5, reward: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/250 [01:47<7:24:15, 107.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "- Basic unit, naphthalene, has a melting point of 80°C.\n",
      "- The presence of a ketone group (C=O) can increase the melting point due to polarity and dipole-dipole interactions, let's estimate an increase of +20°C.\n",
      "- The presence of a hydroxyl group (OH) can lead to hydrogen bonding, which usually has a significant effect on increasing the melting point, approximately +40°C in the context of naphthalene derivatives due to hydrogen bonding.\n",
      "- The molecule has a complex structure with multiple rings and substituents, which can significantly increase the melting point due to increased molecular weight and van der Waals interactions. However, since this is already accounted for in the base value of naphthalene, we will not add any additional increase.\n",
      "- The presence of an ether linkage (OC) can increase the melting point due to the polarity it introduces, let's estimate an increase of +5°C.\n",
      "\n",
      "Summing these contributions gives us a total estimated increase from the base value of naphthalene of 145°C.\n",
      "##Prediction: 225.0\n",
      "actual: 99.5, predicted: 225.0, reward: -1\n",
      "----\n",
      "\n",
      "\n",
      "- Basic unit, pyridine, has a melting point of 20°C. \n",
      "- The presence of the erythroidine structure, which includes a bicyclic system and multiple nitrogen atoms, significantly increases the melting point due to the rigidity of the structure and potential for hydrogen bonding. We can estimate this to contribute around +150 to the melting point. \n",
      "- The presence of the ketone group (C=O) can also increase the melting point due to dipole-dipole interactions and potential hydrogen bonding, estimated at +10. \n",
      "- The presence of the ether linkage (O) can also increase the melting point due to dipole-dipole interactions, estimated at +5. \n",
      "Adding these values to the base pyridine melting point gives us a collective increase.\n",
      "##Prediction: 185.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual: 99.5, predicted: 185.0, reward: -0.71\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#model.config.use_cache = False \n",
    "#model.config.use_cache = True\n",
    "lr_epochs=250\n",
    "#lr_epochs=3\n",
    "\n",
    "n_iterations=0\n",
    "\n",
    "for i in tqdm(range(lr_epochs)):\n",
    "   #ランダムに問題を設定して値を予測させる\n",
    "    train_id=random.randint(n_test,len(dataset))\n",
    "\n",
    "    \n",
    "    #このクラスにgpu cacheが残りがちなので､毎回初期化してメモリ開放しておく\n",
    "    reward_model=None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    reward_model = RewardModel(model,dataset,tokenizer)\n",
    "\n",
    "\n",
    "    #同じ問題に対して､良い結果が来るまで何回か問題を解かせる\n",
    "    for i in range(2):\n",
    "        try:\n",
    "            #報酬､応答､入力の取得\n",
    "            reward,response,input_id=reward_model(train_id)\n",
    "            rewards=[torch.tensor(float(reward))]\n",
    "            query_tensors = [torch.tensor(input_id).reshape(-1)]\n",
    "            response_tensors=[torch.tensor(tokenizer.encode(response)).reshape(-1)]\n",
    "\n",
    "            #モデル更新\n",
    "            stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "            n_iterations+=1\n",
    "        except Exception as e:\n",
    "            #主にcuda out of memoryが起きるので､一旦メモリ開放して､model類を読み込み直す\n",
    "            print(e)\n",
    "            model,ppo_trainer=reload_ppo_model_and_trainer(model,ppo_trainer)\n",
    "\n",
    "            break\n",
    "\n",
    "        #結果がよかったら次の問題に移る\n",
    "        if reward>0.5:\n",
    "            break\n",
    "\n",
    "n_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "#AutoModelForCausalLMWithValueHead classはdirを作っておかないと､save_pretrainedが動かない\n",
    "current_datetime = datetime.now()\n",
    "model_save_path=f\"outputs/ppo{current_datetime}\"\n",
    "os.mkdir(model_save_path)\n",
    "model.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルによる物性値の予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaFlashAttention2(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(\n",
       "                in_features=4096, out_features=11008, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): Linear(\n",
       "                in_features=4096, out_features=11008, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(\n",
       "        in_features=4096, out_features=32000, bias=False\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promlem 1 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:02<01:47,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "##Prediction: 120.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  74.0 predicted:  120.0\n",
      "promlem 2 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:22<10:20, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "To estimate the melting point of 2,3-dichloroquinoxaline, we need to analyze the structural features and their impact on the melting point. Quinoxaline itself has a melting point of around 100 degrees Celsius. The introduction of two chlorine atoms as substituents can increase the melting point due to the increased molecular weight and the ability of chlorine atoms to form hydrogen bonds with nearby molecules. Each chlorine atom can add around +50 degrees Celsius to the melting point.\n",
      "##Prediction: 150.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  152.0 predicted:  150.0\n",
      "promlem 3 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:24<06:14,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "##Prediction: 105.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  100.0 predicted:  105.0\n",
      "promlem 4 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:35<07:06,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "The basic unit for this compound is a phenyl group, which has a melting point of around 80 degrees Celsius. The presence of the alkene functional group (-CH=CH-CH2-) introduces the possibility of pi-pi interactions, which can increase the melting point. Additionally, the two phenyl groups are in a trans configuration, which can also increase the melting point due to the increased steric hindrance between the two groups.\n",
      "##Prediction: 120.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  56.5 predicted:  120.0\n",
      "promlem 5 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:37<04:59,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "##Prediction: 105.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  26.0 predicted:  105.0\n",
      "promlem 6 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [01:21<14:00, 19.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "The target compound n-(1,2-diphenylethyl)nicotinamide contains a phenyl group, which is known to increase the melting point due to the presence of pi-pi interactions. The diphenylethyl group is a branched alkyl chain that may also contribute to the melting point through increased van der Waals interactions. The nicotinamide moiety is a polar functional group that can participate in hydrogen bonding, which may also affect the melting point. Considering the effects of these functional groups: The phenyl group is expected to increase the melting point by approximately 20 degrees. The diphenylethyl group may contribute an additional 5 degrees due to its branching. The nicotinamide moiety may also increase the melting point by approximately 5 degrees due to its polarity. However, the actual value of 130 degrees Celsius suggests that these estimates need to be adjusted. The impact of the phenyl group is aligned with expectations, but the diphenylethyl group's effect is overestimated, and the nicotinamide moiety's effect is underestimated.\n",
      "##Prediction: 115.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  159.0 predicted:  115.0\n",
      "promlem 7 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [01:23<09:50, 13.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "##Prediction: 120.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  109.0 predicted:  120.0\n",
      "promlem 8 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [01:25<07:00, 10.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "##Prediction: 55.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  -99.0 predicted:  55.0\n",
      "promlem 9 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [01:39<07:30, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "The basic unit, acetamide, has a melting point of 120.5 degrees Celsius. The nitro functional group is electron-withdrawing and polar, which can increase the melting point due to increased intermolecular forces. The phenyl group is also polar and capable of hydrogen bonding, which can further increase the melting point. The estimated effect of the nitro group is around +30°C, and the effect of the phenyl group is around +20°C.\n",
      "##Prediction: 150.5\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  93.0 predicted:  150.5\n",
      "promlem 10 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [01:41<05:33,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "##Prediction: 55.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  -92.0 predicted:  55.0\n",
      "promlem 11 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [02:15<10:29, 16.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "The target compound is 4-iodo-2-nitroanisole. The basic unit for comparison is anisole, which has a melting point of 60.5°C. The iodine atom is electronegative and polar, which can increase the melting point due to intermolecular dipole-dipole interactions. The nitro group is electron-withdrawing and can also increase the rigidity of the molecule, leading to a higher melting point. The iodine atom can add approximately +10 degrees Celsius due to its electronegativity and polar nature. The nitro group can add +20 degrees Celsius due to its electron-withdrawing nature. Therefore, the expected adjustments to the melting point due to these functional groups are as follows: base melting point of anisole (+60.5 degrees Celsius), iodine atom (+10 degrees Celsius), and nitro group (+20 degrees Celsius).\n",
      "##Prediction: 70.5\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  97.0 predicted:  70.5\n",
      "promlem 12 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [02:39<11:49, 18.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "The basic unit for consideration is hexene, which has a melting point of 10°C. The addition of two methyl groups (CH3) to the carbon atom at position 2 results in an increase in the melting point due to the increased molecular weight and polarizability of the methyl groups. The (Z) stereochemistry of the molecule also contributes to the melting point, as the Z isomer has a higher melting point than the E isomer due to the increased steric hindrance between the methyl groups. Therefore, I predict:\n",
      "\n",
      "+2°C for the methyl substitution.\n",
      "+5°C for the Z stereochemistry.\n",
      "\n",
      "Total adjustment: +7°C\n",
      "##Prediction: 17.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  -137.4 predicted:  17.0\n",
      "promlem 13 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [02:42<08:26, 13.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "##Prediction: 140.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  203.0 predicted:  140.0\n",
      "promlem 14 / 50\n"
     ]
    }
   ],
   "source": [
    "#予測時のハイパラ\n",
    "n_prompt_examples=3 #何件の例題をprompt tuningで出すか\n",
    "n_max_trials=2  # 値を返さなかったときの再試行の最大数\n",
    "random.seed(0)\n",
    "prediction_results={}\n",
    "\n",
    "\n",
    "res_list=[]\n",
    "for test_id in tqdm(range(n_test)):\n",
    "    print(f\"promlem {test_id+1} / {n_test}\")\n",
    "    for _ in range(n_max_trials):\n",
    "        try:\n",
    "            prompt=generate_question_prompt(dataset,test_id,n_prompt_examples=n_prompt_examples)\n",
    "            reason,value=ask_value(prompt,model,tokenizer)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "\n",
    "        if value is not None:\n",
    "            record=copy.deepcopy(dataset[test_id])\n",
    "            record[\"Test (Predicted reason)\"]=reason\n",
    "            record[\"Test (Predicted value)\"]=value\n",
    "            res_list.append(record)\n",
    "            print(\"actual: \",record[\"mpC\"],\"predicted: \", record[\"Test (Predicted value)\"],)\n",
    "            break\n",
    "prediction_results[n_prompt_examples]=res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "from datetime import datetime\n",
    "import json\n",
    "current_datetime = datetime.now()\n",
    "vmin=-200\n",
    "vmax=300\n",
    "\n",
    "#plot prediction results\n",
    "for n_prompt_examples,records in prediction_results.items():\n",
    "    sel_df=pd.DataFrame(records)\n",
    "    #floatに可能なものは変換\n",
    "    sel_df[\"Test (Predicted value)\"] = pd.to_numeric(sel_df[\"Test (Predicted value)\"], errors='coerce')\n",
    "    sel_df=sel_df[sel_df[\"Test (Predicted value)\"].notnull()]\n",
    "    if len(sel_df)==0:\n",
    "        continue\n",
    "    mse=mean_squared_error(sel_df[\"mpC\"],sel_df[\"Test (Predicted value)\"])\n",
    "\n",
    "    plt.figure()\n",
    "    sns.scatterplot(data=sel_df,x=\"mpC\",y=\"Test (Predicted value)\")\n",
    "    plt.title(f\"n_prompt_examples={n_prompt_examples} MSE={mse:.0f}\")\n",
    "\n",
    "    #x,yの範囲を揃える\n",
    "    plt.xlim(vmin,vmax)\n",
    "    plt.ylim(vmin,vmax)\n",
    "    #対角線を描く\n",
    "    plt.plot([vmin,vmax],[vmin,vmax],color=\"gray\")\n",
    "    formatted_filename = f\"results/model={model_size}_{current_datetime.strftime('%Y%m%d_%H%M%S')}_train={do_train}.png\"\n",
    "    plt.savefig(formatted_filename)\n",
    "    #break\n",
    "\n",
    "save_json_filename=formatted_filename.replace(\".png\",\".json\")\n",
    "with open(save_json_filename,\"w\") as f:\n",
    "    json.dump(prediction_results,fp=f,\n",
    "              indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#スコア\n",
    "print(mean_squared_error(sel_df[\"mpC\"],sel_df[\"Test (Predicted value)\"]))\n",
    "print(mean_absolute_error(sel_df[\"mpC\"],sel_df[\"Test (Predicted value)\"]))\n",
    "print(r2_score(sel_df[\"mpC\"],sel_df[\"Test (Predicted value)\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#回答可能な問題の割合\n",
    "sel_df.shape[0]/n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
