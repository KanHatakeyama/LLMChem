{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Q)分子構造+(R)理由+(A)物性データセットのLLMによる学習と予測\n",
    "- Q&A: 融点データセットを使用\n",
    "- R: 自分自身で考えさせて､正解のデータを学習させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "from llmchem.utils import mk_dir,clean_vram\n",
    "\n",
    "#import clear_output\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset settings\n",
    "n_test=5 #number of testing data\n",
    "n_train_check=5 #number of training data for checking (i.e., checking everything takes too long, so we check only a part of training data)\n",
    "n_GPT_reasoning=30 # number of reasoning data made by GPT\n",
    "n_generation_iterations=10   # trial numbers to generate new self reasoning data\n",
    "max_generations=10\n",
    "\n",
    "#model settings\n",
    "model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "target_modules= [\n",
    "    \"lm_head\",\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate\",\n",
    "    \"w1\",\n",
    "    \"w2\",\n",
    "    \"w3\"\n",
    "]\n",
    "\n",
    "model_name=f\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "target_modules= [\n",
    "    #\"embed_tokens\",\n",
    "    \"lm_head\",\n",
    "    #\"q_proj\",\n",
    "    #\"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    #\"down_proj\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "#LoRA settings\n",
    "r=32\n",
    "lora_alpha=r\n",
    "bit=16\n",
    "#bit=8\n",
    "#bit=4\n",
    "\n",
    "#train settings\n",
    "gradient_checkpointing =False\n",
    "per_device_train_batch_size=1\n",
    "epochs=3\n",
    "lr=10**-5\n",
    "\n",
    "#device settings\n",
    "device_map=\"auto\"\n",
    "\n",
    "#dataset path\n",
    "dataset_path=\"dataset/231225AutoReasoning/240117best_reason_record_30k.csv\"\n",
    "\n",
    "#project path\n",
    "project_dir=\"results/projects/240117test\"\n",
    "\n",
    "#reasoning options\n",
    "error_threshold=30  # if abolute error is smaller than this, add to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_dir(project_dir)\n",
    "mk_dir(project_dir+\"/eval\")\n",
    "mk_dir(project_dir+\"/self_reasoning\")\n",
    "mk_dir(project_dir+\"/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmchem.model import init_model\n",
    "from llmchem.train import train_model\n",
    "from llmchem.eval import eval_model\n",
    "from llmchem.reasoning import self_reasoning\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load base dataset\n",
    "\n",
    "df=pd.read_csv(dataset_path)\n",
    "dataset=df.to_dict(orient=\"records\")\n",
    "random.seed(0)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "base_train_dataset=dataset[:-n_test]\n",
    "train_check_dataset=base_train_dataset[-n_train_check:]\n",
    "example_reasoning_dataset=base_train_dataset[:n_GPT_reasoning]\n",
    "test_dataset=dataset[-n_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-generated reasons: 30\n",
      "All-generated reasons: 30\n",
      "Using fp16 mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 2425.34 examples/s]\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 00:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promlem 1 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [01:02<04:09, 62.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "Heroin's melting point is challenging to predict due to its complex structure, which includes a phenanthrene core, a nitro functional group, and a methyl ester group. The phenanthrene core has a high melting point due to its planarity and the presence of a conjugated double bond. The nitro functional group can increase the melting point due to its electronegativity and the potential for hydrogen bonding. The methyl ester group can also contribute to the melting point due to its dipole moment and the potential for hydrogen bonding. The presence of these functional groups and their interactions with each other and the phenanthrene core can lead to a complex interplay of effects on the melting point.\n",
      "\n",
      "To estimate the melting point of heroin, we need to consider the effects of each functional group and their interactions. The phenanthrene core is estimated to increase the melting point by +40 over a basic hydrocarbon backbone. The nitro functional group can increase the melting point by +10 to +20 degrees due to its electronegativity and potential for hydrogen bonding. The methyl ester group can increase the melting point by +5 to +10 degrees due to its dipole moment and potential for hydrogen bonding. The interactions between these functional groups can also contribute to the melting point, potentially increasing it by +10 to +20 degrees.\n",
      "\n",
      "Considering the complexity of heroin's structure, it is difficult to estimate the melting point accurately. However, based on the available data and the structural features of the molecule, we can estimate the melting point to be around 150-170°C.\n",
      "##Prediction: 155.0 #include <iostream>\n",
      "#include <string>\n",
      "#include\n",
      "actual:  173.0 predicted:  155.0\n",
      "promlem 2 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [01:04<01:21, 27.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "##Prediction: 125.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  96.0 predicted:  125.0\n",
      "promlem 3 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:07<00:31, 15.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "##Prediction: 105.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  -80.0 predicted:  105.0\n",
      "promlem 4 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:09<00:10, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "##Prediction: 105.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  190.0 predicted:  105.0\n",
      "promlem 5 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:25<00:00, 17.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "Theophylline is a xanthine alkaloid with a melting point of 190-195 °C. The molecule has a planar, symmetrical structure with a nitrogen atom at the center of the ring. The oxygen atoms are also symmetrical and are bonded to the nitrogen through single bonds. The carbon atoms are bonded to the nitrogen and oxygen through single bonds. The hydrogen atoms are bonded to the carbon atoms through single bonds. The molecule has a high degree of symmetry, which can contribute to a higher melting point due to the increased molecular weight and the stabilizing effect of the symmetry.\n",
      "##Prediction: 190.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  272.0 predicted:  190.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promlem 1 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:38<02:34, 38.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "\n",
      "The functional groups in this molecule are a methoxy group, a nitro group, and a benzaldehyde group. The methoxy group is polar and hydrogen bonding capable, while the nitro group is highly electronegative and will increase the melting point due to intermolecular dipole-dipole interactions. The benzaldehyde group is a relatively small functional group, but it can still contribute to the melting point due to conjugation and increased polarity.\n",
      "\n",
      "Based on the individual contributions of each functional group, we can estimate the melting point as follows:\n",
      "\n",
      "- Methoxy group: 50 degrees Celsius (based on the average melting point of similar compounds)\n",
      "- Nitro group: 40 degrees Celsius (based on the increased polarity and dipole-dipole interactions)\n",
      "- Benzaldehyde group: 20 degrees Celsius (based on the conjugation effect and increased polarity)\n",
      "\n",
      "Adding these contributions up, we get a predicted melting point of 110 degrees Celsius.\n",
      "##Prediction: 110.0\n",
      "\n",
      "\n",
      "#Problem\n",
      "actual:  99.0 predicted:  110.0\n",
      "promlem 2 / 5\n"
     ]
    }
   ],
   "source": [
    "#Loop: training, evaluation, data generation\n",
    "for generation in range(max_generations):\n",
    "    clear_output()\n",
    "    #prepare train dataset\n",
    "\n",
    "    ## reason data made by GPT4\n",
    "    train_dataset=copy.deepcopy(example_reasoning_dataset)\n",
    "\n",
    "    print(f\"GPT-generated reasons: {len(train_dataset)}\")\n",
    "\n",
    "    ## reason data made by model itself\n",
    "    for path in glob.glob(f\"{project_dir}/self_reasoning/*.json\"):\n",
    "        with open(path) as f:\n",
    "            train_dataset.append(json.load(f))\n",
    "\n",
    "    print(f\"All-generated reasons: {len(train_dataset)}\")\n",
    "    random.shuffle(train_dataset)\n",
    "\n",
    "    #train model\n",
    "    clean_vram()\n",
    "    model=init_model(model_name, r, lora_alpha, target_modules, bit=bit,device_map=device_map)\n",
    "    train_result=train_model(model,tokenizer,train_dataset,\n",
    "                    project_dir=project_dir,\n",
    "                    epochs=epochs,\n",
    "                    lr=lr,\n",
    "                    per_device_train_batch_size=per_device_train_batch_size,\n",
    "                    gradient_checkpointing=gradient_checkpointing,\n",
    "                    )\n",
    "\n",
    "    #eval\n",
    "    train_eval_result=eval_model(model,tokenizer,train_check_dataset,\n",
    "                                f\"{project_dir}/eval\",\n",
    "                                n_prompt_examples=3,\n",
    "                                prompt_dataset=example_reasoning_dataset,\n",
    "                                prefix=f\"train_{generation}\"\n",
    "                                )\n",
    "\n",
    "    test_eval_result=eval_model(model,tokenizer,test_dataset,\n",
    "                                f\"{project_dir}/eval\",\n",
    "                                n_prompt_examples=3,\n",
    "                                prompt_dataset=example_reasoning_dataset,\n",
    "                                prefix=f\"test_{generation}\"\n",
    "                                )\n",
    "\n",
    "    #generate additional training data by self-reasoning\n",
    "    self_reasoning(model,tokenizer,base_train_dataset,\n",
    "                example_reasoning_dataset,project_dir,generation=generation,\n",
    "                n_iterations=n_generation_iterations,\n",
    "                error_threshold=error_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
